{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP\n",
    "\n",
    "***NLP*** or ***Natural Langauge Processing*** is a subdomain of both linguistics and computer science. As a discipline, its primary concern deals with using computation to understand, analyze, and generate natural language data.\n",
    "\n",
    "Today we will be taking a look at an example of *NLP* by conducting a ***sentiment analysis***. Sentiment Analysis is the process of taking some textual data and assigning it some poliarity score (typically: positive, neutral, negative) based upon the lexigraphical features of the text.\n",
    "\n",
    "The example we will be utilizing today involves utilizing *vaderSentiment* a package that was purpose built to conduct sentiment analysis upon social media posts to understand not only the poliarty of social media posts but also the *valence* (i.e. parts of speech that may modify poliarity) of some text. Vader also does some interpretation upoon the sentiment polarity of emojis which makes it particularly valuable in a social media context\n",
    "\n",
    "Concepts:\n",
    "- [NLP](https://en.wikipedia.org/wiki/Natural_language_processing)\n",
    "- [Sentiment Analysis](https://en.wikipedia.org/wiki/Sentiment_analysis#Subjectivity/objectivity_identification)\n",
    "- [vaderSentiment](https://github.com/cjhutto/vaderSentiment#features-and-updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tweepy as tp\n",
    "import nltk\n",
    "import vaderSentiment.vaderSentiment as vd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweepy\n",
    "\n",
    "In this example we will be using tweepy a library designed to act as a rapper to the twitter API. It provides functionality to quickly interact with and extract data such as tweets and user info from twitter.\n",
    "\n",
    "Before you can begin, you'll need to set up an app and get your connection keys/tokens for the twitter API. You can find the docs and figure out how to get your keys by following the first link below.\n",
    "\n",
    "[Twitter API Docs](https://developer.twitter.com/en/docs)\n",
    "\n",
    "[Tweepy Docs](http://docs.tweepy.org/en/latest/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our keys and tokens, remember it goes against best practices to store a\n",
    "# token or password directly in your scripts / notebooks\n",
    "con_key = <consumer_key>\n",
    "con_secret_key = <consumer_secret_key>\n",
    "access_token = <access_token>\n",
    "access_secret_token = <access_secret_token>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate to twitter and generate an API object we can interact with\n",
    "auth = tp.OAuthHandler(con_key, con_secret_key)\n",
    "\n",
    "#set up authentication tokens for our api acess\n",
    "auth.set_access_token(access_token, access_secret_token)\n",
    "\n",
    "# Construct the API instance\n",
    "api = tp.API(auth)\n",
    "\n",
    "# User\n",
    "user = '@elonmusk'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get some details about our twitter user\n",
    "item = api.get_user(user)\n",
    "print(\"name: \" + item.name)\n",
    "print(\"screen_name: \" + item.screen_name)\n",
    "print(\"description: \" + item.description)\n",
    "print(\"statuses_count: \" + str(item.statuses_count))\n",
    "print(\"friends_count: \" + str(item.friends_count))\n",
    "print(\"followers_count: \" + str(item.followers_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize a list to hold all the tweepy Tweets\n",
    "alltweets = []\t\n",
    "\t\n",
    "#make initial request for most recent tweets (200 is the maximum allowed count)\n",
    "new_tweets = api.user_timeline(screen_name=user,count=200)\n",
    "\t\n",
    "#save most recent tweets\n",
    "alltweets.extend(new_tweets)\n",
    "\t\n",
    "#save the id of the oldest tweet less one\n",
    "oldest = alltweets[-1].id - 1\n",
    "\t\n",
    "#keep grabbing tweets until there are no tweets left to grab\n",
    "while len(new_tweets) > 0:\n",
    "\tprint(f\"getting tweets before {oldest}\")\n",
    "\t\t\n",
    "\t#all subsiquent requests use the max_id param to prevent duplicates\n",
    "\tnew_tweets = api.user_timeline(screen_name =user,count=200,max_id=oldest)\n",
    "\t\t\n",
    "\t#save most recent tweets\n",
    "\talltweets.extend(new_tweets)\n",
    "\t\t\n",
    "\t#update the id of the oldest tweet less one\n",
    "\toldest = alltweets[-1].id - 1\n",
    "\t\t\n",
    "\tprint(f\"...{len(alltweets)} tweets downloaded so far\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Dataframe\n",
    "\n",
    "Now that we have pulled our data down from twitter we need to manipulate it in a way to make it easier to process. Each of the *status* objects within  the **alltweets** list has attributes that you can pull out of them. We can access these attributes individually with list comprehensions and build a dataframe. An example is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we build a data frame by using list comprehensions on the tweet status objects\n",
    "elon_df = pd.DataFrame(data={'Tweet':[tweet.text for tweet in alltweets],\n",
    "                             'Timestamp':[tweet.created_at for tweet in alltweets],\n",
    "                             'favorites_count':[tweet.favorite_count for tweet in alltweets],\n",
    "                             'retweet_count':[tweet.retweet_count for tweet in alltweets],\n",
    "                             'tweet_source':[tweet.source for tweet in alltweets],\n",
    "                             'in reply to':[tweet.in_reply_to_screen_name for tweet in alltweets],\n",
    "                             'is_retweet?':[True if tweet.text[0:2] == 'RT' else False for tweet in alltweets],\n",
    "                             'Tweet_Length':[len(tweet.text) for tweet in alltweets]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Vader to generate polarity scores\n",
    "\n",
    "Detailed information on vader can be found here:\n",
    "[vaderSentiment](https://github.com/cjhutto/vaderSentiment#features-and-updates)\n",
    "\n",
    "TLDR;\n",
    "\n",
    "Vader allows us to rate strings of text based upon an inbuilt lexicon. The lexicon's values are modified by:\n",
    "\n",
    "- typical negations (e.g., \"not good\")\n",
    "- use of contractions as negations (e.g., \"wasn't very good\")\n",
    "- conventional use of punctuation to signal increased sentiment intensity (e.g., \"Good!!!\")\n",
    "- conventional use of word-shape to signal emphasis (e.g., using ALL CAPS for words/phrases)\n",
    "- using degree modifiers to alter sentiment intensity (e.g., intensity boosters such as \"very\" and intensity dampeners such as \"kind of\")\n",
    "- understanding many sentiment-laden slang words (e.g., 'sux')\n",
    "- understanding many sentiment-laden slang words as modifiers such as 'uber' or 'friggin' or 'kinda'\n",
    "- understanding many sentiment-laden emoticons such as :) and :D\n",
    "- translating utf-8 encoded emojis such as ðŸ’˜ and ðŸ’‹ and ðŸ˜\n",
    "- understanding sentiment-laden initialisms and acronyms (for example: 'lol')\n",
    "\n",
    "\n",
    "### Getting Started\n",
    "\n",
    "We build an analyzer instance and then we can apply that to string values to pull out some polarity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example of vader sentiment analyzer at work\n",
    "analyzer = vd.SentimentIntensityAnalyzer()\n",
    "analyzer.polarity_scores(elon_df['Tweet'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate scores for all tweets in the dataframe\n",
    "elon_df['positive_score'] = [analyzer.polarity_scores(tweet)['pos'] for tweet in elon_df['Tweet']]\n",
    "elon_df['neutral_score']  = [analyzer.polarity_scores(tweet)['neu'] for tweet in elon_df['Tweet']]\n",
    "elon_df['negative_score'] = [analyzer.polarity_scores(tweet)['neg'] for tweet in elon_df['Tweet']]\n",
    "elon_df['compound_score'] = [analyzer.polarity_scores(tweet)['compound'] for tweet in elon_df['Tweet']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save our processed data for later\n",
    "elon_df.to_csv('elon_tweet.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do some time processing\n",
    "elon_df['Month'] = [time.month_name() for time in elon_df['Timestamp']]\n",
    "elon_df['Year'] = [time.year for time in elon_df['Timestamp']]\n",
    "elon_df['date'] = pd.to_datetime(elon_df['Timestamp'], format='%m-%Y').dt.strftime('%m-%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the mean scores for each month\n",
    "average_month =  elon_df.groupby('date',axis=0).agg({'compound_score':'mean',\n",
    "                                                     'positive_score':'mean',\n",
    "                                                     'neutral_score':'mean',\n",
    "                                                     'negative_score':'mean'})\n",
    "average_month = pd.concat([average_month.iloc[4:],average_month.iloc[0:4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the max scores for each month\n",
    "max_month =  elon_df.groupby('date',axis=0).agg({'compound_score':'max',\n",
    "                                                 'positive_score':'max',\n",
    "                                                 'neutral_score':'max',\n",
    "                                                 'negative_score':'max'})\n",
    "max_month = pd.concat([max_month.iloc[4:],max_month.iloc[0:4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(elon_df.query('compound_score>=.5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_month.plot(figsize=(12,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_month.plot(figsize=(16,8))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
